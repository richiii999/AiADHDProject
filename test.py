import cv2
import sys
import cv2 as cv
import numpy as np
import mediapipe as mp
import math
import socket
import argparse
import time
import csv
import threading
from datetime import datetime
import os
import asyncio
from Sensors.PythonFaceTracker.AngleBuffer import AngleBuffer
from Sensors.GazeTracking.gaze_tracking import GazeTracking
from multiprocessing import Process


#from gaze_tracking import GazeTracking

webcam = cv2.VideoCapture(0)

async def gaze_estimator(camera):
    gaze = GazeTracking()
    webcam = camera

    while True:
        await asyncio.sleep(0.01)
        # We get a new frame from the webcam
        _, frame = webcam.read()

        # We send this frame to GazeTracking to analyze it
        gaze.refresh(frame)

        frame = gaze.annotated_frame()
        text = ""

        if gaze.is_blinking():
            text = "Blinking"
        elif gaze.is_right():
            text = "Looking right"
        elif gaze.is_left():
            text = "Looking left"
        elif gaze.is_center():
            text = "Looking center"

        cv2.putText(frame, text, (90, 60), cv2.FONT_HERSHEY_DUPLEX, 1.6, (147, 58, 31), 2)

        left_pupil = gaze.pupil_left_coords()
        right_pupil = gaze.pupil_right_coords()
        cv2.putText(frame, "Left pupil:  " + str(left_pupil), (90, 130), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
        cv2.putText(frame, "Right pupil: " + str(right_pupil), (90, 165), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)

        cv2.imshow("Demo", frame)

        if cv2.waitKey(1) == 27:
            break
    
    webcam.release()
    cv2.destroyAllWindows()




#THIS is the face estimator
async def face_estimator(camera):
    frametime = 0.01 # How long between each frame (in sec)
    lookThreshhold = 20 # How many consecutive frames before a distraction is triggered?

    # ADHD Vars
    distractionCount = 0 # Counts how many distractions have occoured this session
    lookCounter = 0 # Counts how many consecutive frames face_look is not 'Forward'
    #-----------------------------------------------------------------------------------------------------------------------------------

    ## User-Specific Measurements
    # USER_FACE_WIDTH: The horizontal distance between the outer edges of the user's cheekbones in millimeters. 
    # This measurement is used to scale the 3D model points for head pose estimation.
    # Measure your face width and adjust the value accordingly.
    USER_FACE_WIDTH = 140  # [mm]

    ## Camera Parameters (not currently used in calculations)
    # NOSE_TO_CAMERA_DISTANCE: The distance from the tip of the nose to the camera lens in millimeters.
    # Intended for future use where accurate physical distance measurements may be necessary.
    NOSE_TO_CAMERA_DISTANCE = 600  # [mm]

    # ENABLE_HEAD_POSE: Enable the head position and orientation estimator.
    ENABLE_HEAD_POSE = True

    ## Blink Detection Parameters
    # SHOW_ON_SCREEN_DATA: If True, display blink count and head pose angles on the video feed.
    SHOW_ON_SCREEN_DATA = True

    ## Head Pose Estimation Landmark Indices
    # These indices correspond to the specific facial landmarks used for head pose estimation.
    LEFT_EYE_IRIS = [474, 475, 476, 477]
    RIGHT_EYE_IRIS = [469, 470, 471, 472]
    LEFT_EYE_OUTER_CORNER = [33]
    LEFT_EYE_INNER_CORNER = [133]
    RIGHT_EYE_OUTER_CORNER = [362]
    RIGHT_EYE_INNER_CORNER = [263]
    RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]
    LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]
    NOSE_TIP_INDEX = 4
    CHIN_INDEX = 152
    LEFT_EYE_LEFT_CORNER_INDEX = 33
    RIGHT_EYE_RIGHT_CORNER_INDEX = 263
    LEFT_MOUTH_CORNER_INDEX = 61
    RIGHT_MOUTH_CORNER_INDEX = 291

    ## MediaPipe Model Confidence Parameters
    # These thresholds determine how confidently the model must detect or track to consider the results valid.
    MIN_DETECTION_CONFIDENCE = 0.8
    MIN_TRACKING_CONFIDENCE = 0.8

    ## Angle Normalization Parameters
    # MOVING_AVERAGE_WINDOW: The number of frames over which to calculate the moving average for smoothing angles.
    MOVING_AVERAGE_WINDOW = 10

    # Initial Calibration Flags
    # initial_pitch, initial_yaw, initial_roll: Store the initial head pose angles for calibration purposes.
    # calibrated: A flag indicating whether the initial calibration has been performed.
    initial_pitch, initial_yaw, initial_roll = None, None, None
    calibrated = False

    # User-configurable parameters
    PRINT_DATA = True  # Enable/disable data printing
    DEFAULT_WEBCAM = 0  # Default webcam number
    SHOW_ALL_FEATURES = True  # Show all facial landmarks if True

    # Server configuration
    SERVER_IP = "127.0.0.1"  # Set the server IP address (localhost)
    SERVER_PORT = 7070  # Set the server port
    # SERVER_ADDRESS: Tuple containing the SERVER_IP and SERVER_PORT for UDP communication.
    SERVER_ADDRESS = (SERVER_IP, SERVER_PORT)

    # Command-line arguments for camera source
    parser = argparse.ArgumentParser(description="Eye Tracking Application")
    parser.add_argument("-c", "--camSource", help="Source of camera", default=str(DEFAULT_WEBCAM))
    args = parser.parse_args()

    # Iris and eye corners landmarks indices
    LEFT_IRIS = [474, 475, 476, 477]
    RIGHT_IRIS = [469, 470, 471, 472]
    L_H_LEFT = [33]  # Left eye Left Corner
    L_H_RIGHT = [133]  # Left eye Right Corner
    R_H_LEFT = [362]  # Right eye Left Corner
    R_H_RIGHT = [263]  # Right eye Right Corner

    # Blinking Detection landmark's indices.
    # P0, P3, P4, P5, P8, P11, P12, P13
    RIGHT_EYE_POINTS = [33, 160, 159, 158, 133, 153, 145, 144]
    LEFT_EYE_POINTS = [362, 385, 386, 387, 263, 373, 374, 380]

    # Face Selected points indices for Head Pose Estimation
    _indices_pose = [1, 33, 61, 199, 263, 291]

    # Server address for UDP socket communication
    SERVER_ADDRESS = (SERVER_IP, 7070)


    # Function to calculate vector position
    def vector_position(point1, point2):
        x1, y1 = point1.ravel()
        x2, y2 = point2.ravel()
        return x2 - x1, y2 - y1


    def euclidean_distance_3D(points):
        # Get the three points.
        P0, P3, P4, P5, P8, P11, P12, P13 = points

        # Calculate the numerator.
        numerator = (
            np.linalg.norm(P3 - P13) ** 3
            + np.linalg.norm(P4 - P12) ** 3
            + np.linalg.norm(P5 - P11) ** 3
        )

        # Calculate the denominator.
        denominator = 3 * np.linalg.norm(P0 - P8) ** 3

        # Calculate the distance.
        distance = numerator / denominator

        return distance

    def estimate_head_pose(landmarks, image_size):
        # Scale factor based on user's face width (assumes model face width is 150mm)
        scale_factor = USER_FACE_WIDTH / 150.0
        # 3D model points.
        model_points = np.array([
            (0.0, 0.0, 0.0),             # Nose tip
            (0.0, -330.0 * scale_factor, -65.0 * scale_factor),        # Chin
            (-225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),     # Left eye left corner
            (225.0 * scale_factor, 170.0 * scale_factor, -135.0 * scale_factor),      # Right eye right corner
            (-150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor),    # Left Mouth corner
            (150.0 * scale_factor, -150.0 * scale_factor, -125.0 * scale_factor)      # Right mouth corner
        ])
        

        # Camera internals
        focal_length = image_size[1]
        center = (image_size[1]/2, image_size[0]/2)
        camera_matrix = np.array(
            [[focal_length, 0, center[0]],
            [0, focal_length, center[1]],
            [0, 0, 1]], dtype = "double"
        )

        # Assuming no lens distortion
        dist_coeffs = np.zeros((4,1))

        # 2D image points from landmarks, using defined indices
        image_points = np.array([
            landmarks[NOSE_TIP_INDEX],            # Nose tip
            landmarks[CHIN_INDEX],                # Chin
            landmarks[LEFT_EYE_LEFT_CORNER_INDEX],  # Left eye left corner
            landmarks[RIGHT_EYE_RIGHT_CORNER_INDEX],  # Right eye right corner
            landmarks[LEFT_MOUTH_CORNER_INDEX],      # Left mouth corner
            landmarks[RIGHT_MOUTH_CORNER_INDEX]      # Right mouth corner
        ], dtype="double")


            # Solve for pose
        (success, rotation_vector, translation_vector) = cv.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv.SOLVEPNP_ITERATIVE)

        # Convert rotation vector to rotation matrix
        rotation_matrix, _ = cv.Rodrigues(rotation_vector)

        # Combine rotation matrix and translation vector to form a 3x4 projection matrix
        projection_matrix = np.hstack((rotation_matrix, translation_vector.reshape(-1, 1)))

        # Decompose the projection matrix to extract Euler angles
        _, _, _, _, _, _, euler_angles = cv.decomposeProjectionMatrix(projection_matrix)
        pitch, yaw, roll = euler_angles.flatten()[:3]


        # Normalize the pitch angle
        pitch = normalize_pitch(pitch)

        return pitch, yaw, roll

    def normalize_pitch(pitch):
        """
        Normalize the pitch angle to be within the range of [-90, 90].

        Args:
            pitch (float): The raw pitch angle in degrees.

        Returns:
            float: The normalized pitch angle.
        """
        # Map the pitch angle to the range [-180, 180]
        if pitch > 180:
            pitch -= 360

        # Invert the pitch angle for intuitive up/down movement
        pitch = -pitch

        # Ensure that the pitch is within the range of [-90, 90]
        if pitch < -90:
            pitch = -(180 + pitch)
        elif pitch > 90:
            pitch = 180 - pitch
            
        pitch = -pitch

        return pitch

    if PRINT_DATA: head_pose_status = "enabled" if ENABLE_HEAD_POSE else "disabled"

    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=MIN_DETECTION_CONFIDENCE,
        min_tracking_confidence=MIN_TRACKING_CONFIDENCE,
    )

    cam_source = int(args.camSource)
    cap = camera # This is it and how it works 

    #print("THIS IS MY SHITS  \n")
    #print(cap)

    #print("I truly and honestly hope this works \n")
    # Initializing socket for data transmission
    iris_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    # Main loop for video capture and processing
    try:
        angle_buffer = AngleBuffer(size=MOVING_AVERAGE_WINDOW)  # Adjust size for smoothing

        while True:
            await asyncio.sleep(0.01)
            ret, frame = cap.read()
            if not ret:
                break

            # Flipping the frame for a mirror effect
            # I think we better not flip to correspond with real world... need to make sure later...
            #frame = cv.flip(frame, 1)
            rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
            img_h, img_w = frame.shape[:2]
            results = mp_face_mesh.process(rgb_frame)

            if results.multi_face_landmarks:
                mesh_points = np.array(
                    [
                        np.multiply([p.x, p.y], [img_w, img_h]).astype(int)
                        for p in results.multi_face_landmarks[0].landmark
                    ]
                )

                # Get the 3D landmarks from facemesh x, y and z(z is distance from 0 points)
                # just normalize values
                mesh_points_3D = np.array(
                    [[n.x, n.y, n.z] for n in results.multi_face_landmarks[0].landmark]
                )
                # getting the head pose estimation 3d points
                head_pose_points_3D = np.multiply(
                    mesh_points_3D[_indices_pose], [img_w, img_h, 1]
                )
                head_pose_points_2D = mesh_points[_indices_pose]

                # collect nose three dimension and two dimension points
                nose_3D_point = np.multiply(head_pose_points_3D[0], [1, 1, 3000])
                nose_2D_point = head_pose_points_2D[0]

                # create the camera matrix
                focal_length = 1 * img_w

                cam_matrix = np.array(
                    [[focal_length, 0, img_h / 2], [0, focal_length, img_w / 2], [0, 0, 1]]
                )

                # The distortion parameters
                dist_matrix = np.zeros((4, 1), dtype=np.float64)

                head_pose_points_2D = np.delete(head_pose_points_3D, 2, axis=1)
                head_pose_points_3D = head_pose_points_3D.astype(np.float64)
                head_pose_points_2D = head_pose_points_2D.astype(np.float64)
                # Solve PnP
                success, rot_vec, trans_vec = cv.solvePnP(
                    head_pose_points_3D, head_pose_points_2D, cam_matrix, dist_matrix
                )
                # Get rotational matrix
                rotation_matrix, jac = cv.Rodrigues(rot_vec)

                # Get angles
                angles, mtxR, mtxQ, Qx, Qy, Qz = cv.RQDecomp3x3(rotation_matrix)

                # Get the y rotation degree
                angle_x = angles[0] * 360
                angle_y = angles[1] * 360
                z = angles[2] * 360

                # if angle cross the values then
                threshold_angle = 10
                # See where the user's head tilting
                if angle_y < -threshold_angle:face_looks = "Left"
                elif angle_y > threshold_angle:face_looks = "Right"
                elif angle_x < -threshold_angle:face_looks = "Down"
                elif angle_x > threshold_angle:face_looks = "Up"
                else:face_looks = "Forward"
                if SHOW_ON_SCREEN_DATA:
                    cv.putText(
                        frame,
                        f"Face Looking at {face_looks}",
                        (img_w - 400, 80),
                        cv.FONT_HERSHEY_TRIPLEX,
                        0.8,
                        (0, 255, 0),
                        2,
                        cv.LINE_AA,
                    )
                # Display the nose direction
                nose_3d_projection, jacobian = cv.projectPoints(
                    nose_3D_point, rot_vec, trans_vec, cam_matrix, dist_matrix
                )

                p1 = nose_2D_point
                p2 = (
                    int(nose_2D_point[0] + angle_y * 10),
                    int(nose_2D_point[1] - angle_x * 10),
                )

                cv.line(frame, p1, p2, (255, 0, 255), 3)
                
                # Display all facial landmarks if enabled
                if SHOW_ALL_FEATURES:
                    for point in mesh_points:
                        cv.circle(frame, tuple(point), 1, (0, 255, 0), -1)
                # Process and display eye features
                (l_cx, l_cy), l_radius = cv.minEnclosingCircle(mesh_points[LEFT_EYE_IRIS])
                (r_cx, r_cy), r_radius = cv.minEnclosingCircle(mesh_points[RIGHT_EYE_IRIS])
                center_left = np.array([l_cx, l_cy], dtype=np.int32)
                center_right = np.array([r_cx, r_cy], dtype=np.int32)

                # Highlighting the irises and corners of the eyes
                cv.circle(frame, center_left, int(l_radius), (255, 0, 255), 2, cv.LINE_AA)  # Left iris
                cv.circle(frame, center_right, int(r_radius), (255, 0, 255), 2, cv.LINE_AA)  # Right iris
                cv.circle(frame, mesh_points[LEFT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA)  # Left eye right corner
                cv.circle(frame, mesh_points[LEFT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA)  # Left eye left corner
                cv.circle(frame, mesh_points[RIGHT_EYE_INNER_CORNER][0], 3, (255, 255, 255), -1, cv.LINE_AA)  # Right eye right corner
                cv.circle(frame, mesh_points[RIGHT_EYE_OUTER_CORNER][0], 3, (0, 255, 255), -1, cv.LINE_AA)  # Right eye left corner

                # Calculating relative positions
                l_dx, l_dy = vector_position(mesh_points[LEFT_EYE_OUTER_CORNER], center_left)
                r_dx, r_dy = vector_position(mesh_points[RIGHT_EYE_OUTER_CORNER], center_right)

                # Printing data if enabled
                if PRINT_DATA: # ADHD distraction detection disables printing for output
                    if ENABLE_HEAD_POSE:
                        pitch, yaw, roll = estimate_head_pose(mesh_points, (img_h, img_w))
                        angle_buffer.add([pitch, yaw, roll])
                        pitch, yaw, roll = angle_buffer.get_average()

                        # Set initial angles on first successful estimation or recalibrate
                        if initial_pitch is None or (key == ord('c') and calibrated):
                            initial_pitch, initial_yaw, initial_roll = pitch, yaw, roll
                            calibrated = True
                            if PRINT_DATA:
                                # print("Head pose recalibrated.")
                                pass

                        # Adjust angles based on initial calibration
                        if calibrated:
                            pitch -= initial_pitch
                            yaw -= initial_yaw
                            roll -= initial_roll
                        

                # Sending data through socket
                timestamp = int(time.time() * 1000)  # Current timestamp in milliseconds
                # Create a packet with mixed types (int64 for timestamp and int32 for the rest)
                packet = np.array([timestamp], dtype=np.int64).tobytes() + np.array([l_cx, l_cy, l_dx, l_dy], dtype=np.int32).tobytes()

                SERVER_ADDRESS = ("127.0.0.1", 7070)
                iris_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                iris_socket.sendto(packet, SERVER_ADDRESS)


                # ADHD Distraction Detection Logic
                time.sleep(frametime) # Live video is not needed, so only capture camera every so often (<1 sec per frame)
                if face_looks != 'Forward':
                    lookCounter += 1
                    if lookCounter >= lookThreshhold:
                        distractionCount += 1
                        print(f"The user seems distracted")
                        lookCounter = 0
                else: lookCounter = 0


            # Displaying the processed frame
            cv.imshow("Eye Tracking", frame)
            # Handle key presses
            key = cv.waitKey(1) & 0xFF

            # Calibrate on 'c' key press
            if key == ord('c'): initial_pitch, initial_yaw, initial_roll = pitch, yaw, roll
            
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        # Releasing camera and closing windows
        cap.release()
        cv.destroyAllWindows()
        iris_socket.close()


"""
thread1 = threading.Thread(target=face_estimator(webcam))
thread1.start()
thread2 = threading.Thread(target=gaze_estimator(webcam))
thread2.start()"""

# Welp this doesn't really work that well tbh. It only runs the first function and doesn't stop running it
# This doesn't allow both of them to switch simultaneously. It only allows one of them.


# TODO Welp it says something about passing in a VIRTUAL CAMERA for both instances to run at the same time.
# Gonna try it and see how it goes

async def multi_thread_this():
    run_multiple_tasks_at_once = await asyncio.gather(face_estimator(webcam), gaze_estimator(webcam))

loop = asyncio.get_event_loop()
loop.run_until_complete(multi_thread_this())

